---
title: "Introduction to Bayesian kernel machine regression"
author: "Jennifer F. Bobb"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to Bayesian kernel machine regression}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r options, echo=FALSE, message=FALSE}
## if the current working directory is the directory where this file is located rather than the project directory, set the working directory to the project directory
if(grepl("vignettes", getwd())) knitr::opts_knit$set(root.dir = paste(getwd(), "..", sep = "/"))
knitr::opts_chunk$set(fig.width = 5, fig.height = 3, message = FALSE)
```

```{r setup, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
##library(bkmr)
devtools::document()
##devtools::load_all()
```

Kernel machine regression (KMR), also called Gaussian process regression, is a popular tool in the machine learning literature. The main idea behind KMR is to flexibly model the relationship between a large number of variables and a particular outcome (dependent variable). The general modeling framework we consider here is 

$$
g(E(Y_i)) = h(z_{i1}, \ldots, z_{iM}) + \beta{\bf x}_i, \quad i = 1, \ldots, n
$$
where $g$ is a monotonic link function, $h$ is a flexible function of the predictor variables $z_{i1}, \ldots, z_{iM}$, and ${\bf x}$ is a vector of covariates assumed to have a linear relationship with the outcome ($\beta$ is the corresponding vector of coefficients). In settings where there are a large number of predictor variables, or when the predictor-response function is complex, potentially nonlinear and non-additive relation, it may be challenging to specify a set of basis functions to represent $h$. In these settings, an alternative way to characterize $h$ is through a kernel machine representation.

Inference for the KMR model can be achieved using both frequentest and Bayesian methods. Liu, Lin, and Ghosh (2007) showed that the model where $Y_i$ is a assumed to be continuous could be expressed as a linear mixed model [REF], and Liu etc (2008) applied similar reasoning to show that the model where $Y_i$ is binary could be fit using a generalized linear mixed model framework [REF]. Stavitsky et al conducted Bayesian inference for KMR, and Bobb et al (2015) considered the context of multi-pollutant mixtures, where the predictor variables $z_1, \ldots, z_M$ may be highly correlated or have some known structure (e.g., air pollutants arising from the same source). 

This package includes functions for conducting both frequentist and Bayesian inference for the model above. In this vignette we focus on Bayesian inference (the vignette `kmr` illustrates functions implementing frequentist methods).

Several software packages are available for fitting KMR models (see http://www.gaussianprocess.org/#code for a list of some of the options). These packages have many functionalities, including a range of kernel functions (e.g., linear, quadratic, Gaussian), outcome types (e.g., continuous, categorical, count), and fitting algorithms including approximate methods (e.g., full likelihood, variational Bayes). Because most of these have been developed and used in the machine learning context, their focus has primarily been for prediction and classification. 



<!-- 
Briefly, a kernel machine representation for $h$ depends on specifying a kernel function
--> 


## Simulate some data

To illustrate the main features of the R package `bkmr`, let's first generate some data. We have built in a few functions directly into the R package for this purpose.

```{r simdata}
set.seed(111)
dat <- SimData(n = 50, M = 4)
y <- dat$y
Z <- dat$Z
X <- dat$X
```

Let's view the true predictor-response function used to generate the data
```{r plot, fig.height=2.5, fig.width=2.5}
z1 <- seq(min(dat$Z[, 1]), max(dat$Z[, 1]), length = 20)
z2 <- seq(min(dat$Z[, 2]), max(dat$Z[, 2]), length = 20)
hgrid.true <- outer(z1, z2, function(x,y) apply(cbind(x,y), 1, dat$HFun))

res <- persp(z1, z2, hgrid.true, theta = 30, phi = 20, expand = 0.5, col = "lightblue", xlab = "", ylab = "", zlab = "")
#points(trans3d(dat$Z[, 1], dat$Z[, 2], dat$h, pmat = res), col = "red", pch = 19, cex = 0.5)
```


## Conduct some exploratory data analysis

```{r cormat}
PlotCorr(Z, print.vals = TRUE)
```

## Prior distributions for the $r_m$ parameters

```{r investigate prior}
priorfits <- InvestigatePrior(y = y, Z = Z, X = X)
```

Here we plot the estimated univariate predictor-response relationships for `r_m` fixed to different values.
```{r investigate prior 2, fig.height=6, fig.width=6}
PlotPriorFits(y = y, Z = Z, X = X, 
              fits = priorfits, which.q = c(1,3,4,6))
```

## Fit BKMR 

```{r fit orig}
set.seed(111)
runtime <- system.time(
    fitkm <- kmbayes(y = y, Z = Z, X = X, iter = 10000, verbose = FALSE, varsel = TRUE)
)
```

## Investigate model convergence

Let's visually inspect the trace plots, showing how various parameter values change as the sampler runs.

```{r trace plots, fig.height=2.5, fig.width=6}
TracePlot(fit = fitkm, par = "beta")
TracePlot(fit = fitkm, par = "sigsq.eps")
TracePlot(fit = fitkm, par = "r", comp = 1)
TracePlot(fit = fitkm, par = "h", comp = 20)
```

## Summarize output

### Summary statistics of the predictor-response function

Compute the overall effect of the mixture, by comparing when all predictors are at a particular percentile to when all are at the 50th percentile.

```{r overall}
risks.overall.approx <- OverallRiskSummaries(fit = fitkm, y = y, Z = Z, X = X)
```

```{r plot overall}
library(ggplot2)
ggplot(risks.overall.approx, aes(quantile, est, ymin = est - 1.96*se, ymax = est + 1.96*se)) + 
    geom_pointrange()
```

Compute summary statistics summarizing the single predictor health effects; compare risk when a single predictor in $h$ is at the 75th versus 25th percentile, when all of the other predictors are fixed at a particular percentile. We refer to this as the single-predictor health risks.

```{r single var}
risks.singvar <- SingVarRiskSummaries(fit = fitkm, y = y, Z = Z, X = X)
```

```{r plot single var}
ggplot(risks.singvar, aes(variable, est, ymin = est - 1.96*se, ymax = est + 1.96*se, 
                          col = q.fixed)) + 
    geom_pointrange(position = position_dodge(width = 0.75)) + coord_flip()
```

Compute summary statistics summarizing interaction; compare the single-predictor health risks when all of the other predictors in $Z$ are fixed to their 75th percentile to when all of the other predictors in $Z$ are fixed to their 25th percentile.

```{r int}
risks.int <- SingVarIntSummaries(fit = fitkm, y = y, Z = Z, X = X)
```

If the 95% credible interval does not cover zero for a particular predictor, this suggests that that predictor may interact with one of the other predictors.
```{r plot int}
ggplot(risks.int, aes(variable, est, ymin = est - 1.96*se, ymax = est + 1.96*se)) + 
    geom_hline(y_intercept = 0, col = "brown", lty = 2) +
    geom_pointrange()
```

### Plot the predictor-response function

Single-variable predictor-response function for other predictors fixed at their 50th percentile.

```{r pred-resp}
pred.resp.univar <- PredictorResponseUnivar(fit = fitkm, y = y, Z = Z, X = X)
```

```{r plot pred-resp, fig.height=4, fig.width=6}
ggplot(pred.resp.univar, aes(z, est, ymin = est - 1.96*se, ymax = est + 1.96*se)) + 
    geom_smooth(stat = "identity") + 
    facet_wrap(~ variable)
```

Two-variable predictor-response function for other predictors fixed at their 50th percentile.

```{r pred-resp2, message=FALSE}
pred.resp.bivar <- PredictorResponseBivar(fit = fitkm, y = y, Z = Z, X = X, 
                                          min.plot.dist = 1)
```

Sometimes the image plots are hard to visualize:
```{r plot pred-resp2, fig.height=5, fig.width=6.5}
ggplot(pred.resp.bivar, aes(z1, z2)) + 
    geom_raster(aes(fill = est)) + 
    facet_grid(variable1 ~ variable2)
```

Another option for investigating bivariate predictor-response relations is to plot the predictor-response function of a single predicor in $Z$ for the second predictor in $Z$ fixed at various quantiles.
```{r pred-resp2 opt2, message=FALSE}
pred.resp.bivar.levels <- PredictorResponseBivarLevels(pred.resp.df = pred.resp.bivar, 
                                                       Z = Z, qs = c(0.25, 0.5, 0.75))
```

```{r plot pred-resp2 opt2, fig.height=5, fig.width=6.5, warning=FALSE}
ggplot(pred.resp.bivar.levels, aes(z1, est)) + 
    geom_smooth(aes(col = quantile), stat = "identity") + 
    facet_grid(variable1 ~ variable2) +
    ggtitle("f(z1 | quantiles of z2)")
```

































