---
title: "Introduction to Bayesian kernel machine regression"
author: "Jennifer F. Bobb"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to Bayesian kernel machine regression}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r options, echo=FALSE, message=FALSE, warning=FALSE}
## if the current working directory is the directory where this file is located rather than the project directory, set the working directory to the project directory
if(grepl("vignettes", getwd())) knitr::opts_knit$set(root.dir = paste(getwd(), "..", sep = "/"))
knitr::opts_chunk$set(fig.width = 5, fig.height = 3, message = FALSE)
```

```{r setup, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, results='hide'}
##library(bkmr)
suppressMessages(devtools::document())
##devtools::load_all()
```

Kernel machine regression (KMR), also called Gaussian process regression, is a popular tool in the machine learning literature. The main idea behind KMR is to flexibly model the relationship between a large number of variables and a particular outcome (dependent variable). The general modeling framework we consider here is 

$$
g(E(Y_i)) = h(z_{i1}, \ldots, z_{iM}) + \beta{\bf x}_i, \quad i = 1, \ldots, n
$$
where $g$ is a monotonic link function, $h$ is a flexible function of the predictor variables $z_{i1}, \ldots, z_{iM}$, and ${\bf x}$ is a vector of covariates assumed to have a linear relationship with the outcome ($\beta$ is the corresponding vector of coefficients). In settings where there are a large number of predictor variables, or when the predictor-response function is complex, potentially nonlinear and non-additive relation, it may be challenging to specify a set of basis functions to represent $h$. In these settings, an alternative way to characterize $h$ is through a kernel machine representation.

This package includes functions for conducting Bayesian inference for the model above. The main function `kmbayes()` fits the model. We also provide several functions to summarize the model output in different ways and to visually display the results.

## Simulate some data

To illustrate the main features of the R package `bkmr`, let's first generate some data. We have built in a few functions directly into the R package for this purpose.

```{r simdata}
set.seed(111)
dat <- SimData(n = 50, M = 4)
y <- dat$y
Z <- dat$Z
X <- dat$X
```

Let's view the true predictor-response function used to generate the data
```{r plot, fig.height=3.5, fig.width=3.5}
z1 <- seq(min(dat$Z[, 1]), max(dat$Z[, 1]), length = 20)
z2 <- seq(min(dat$Z[, 2]), max(dat$Z[, 2]), length = 20)
hgrid.true <- outer(z1, z2, function(x,y) apply(cbind(x,y), 1, dat$HFun))

res <- persp(z1, z2, hgrid.true, theta = 30, phi = 20, expand = 0.5, 
             col = "lightblue", xlab = "", ylab = "", zlab = "")
#points(trans3d(dat$Z[, 1], dat$Z[, 2], dat$h, pmat = res), col = "red", pch = 19, cex = 0.5)
```

## Fit BKMR 

To fit the BKMR model, we use the `kmbayes` function. This function implements the Markov chain Monte Carlo (MCMC) sampler described in the paper. The argument `iter` indicates the number of iterations of the MCMC sampler; `verbose` indicates whether interim output summarizing the progress of the model fitting should be printed; and `varsel` indicates whether to conduct variable selection on the predictors `z_{im}`.
```{r fit orig}
set.seed(111)
runtime <- system.time(
    fitkm <- kmbayes(y = y, Z = Z, X = X, iter = 10000, verbose = FALSE, varsel = TRUE)
)
```

## Investigate model convergence

Let's visually inspect the trace plots, showing how various parameter values change as the sampler runs.

```{r trace plots, fig.height=2.5, fig.width=6}
TracePlot(fit = fitkm, par = "beta")
TracePlot(fit = fitkm, par = "sigsq.eps")
TracePlot(fit = fitkm, par = "r", comp = 1)
TracePlot(fit = fitkm, par = "h", comp = 20)
```

## Estimated posterior inclusion probabilities

Because we fit the model with variable selection (we set the argument `varsel` to be `TRUE`), we can estimate the posterior inclusion probability (PIP) for each of the predictors `z_{im}`.

```{r PIPs}
ExtractPIPs(fitkm)
```


## Summarize output

We next go through the different functions included in the `bkmr` package that can be used to summarize the model output. These include functions to calculate summary statistics of the predictor-response function and to visualize different cross-sections of the high-dimensional exposure-response surface.

### Summary statistics of the predictor-response function

Compute the overall effect of the mixture, by comparing when all predictors are at a particular percentile to when all are at the 50th percentile.

```{r overall}
risks.overall.approx <- OverallRiskSummaries(fit = fitkm, y = y, Z = Z, X = X)
```

```{r plot overall}
library(ggplot2)
ggplot(risks.overall.approx, aes(quantile, est, ymin = est - 1.96*se, ymax = est + 1.96*se)) + 
    geom_pointrange()
```

Compute summary statistics summarizing the single predictor health effects; compare risk when a single predictor in $h$ is at the 75th versus 25th percentile, when all of the other predictors are fixed at a particular percentile. We refer to this as the single-predictor health risks.

```{r single var}
risks.singvar <- SingVarRiskSummaries(fit = fitkm, y = y, Z = Z, X = X)
```

```{r plot single var}
ggplot(risks.singvar, aes(variable, est, ymin = est - 1.96*se, ymax = est + 1.96*se, 
                          col = q.fixed)) + 
    geom_pointrange(position = position_dodge(width = 0.75)) + coord_flip()
```

Compute summary statistics summarizing interaction; compare the single-predictor health risks when all of the other predictors in `Z` are fixed to their 75th percentile to when all of the other predictors in `Z` are fixed to their 25th percentile.

```{r int}
risks.int <- SingVarIntSummaries(fit = fitkm, y = y, Z = Z, X = X)
```

If the 95% credible interval does not cover zero for a particular predictor, this suggests that that predictor may interact with one of the other predictors.
```{r plot int}
ggplot(risks.int, aes(variable, est, ymin = est - 1.96*se, ymax = est + 1.96*se)) + 
    geom_hline(y_intercept = 0, col = "brown", lty = 2) +
    geom_pointrange()
```

### Plot the predictor-response function

Single-variable predictor-response function for other predictors fixed at their 50th percentile.

```{r pred-resp}
pred.resp.univar <- PredictorResponseUnivar(fit = fitkm, y = y, Z = Z, X = X)
```

```{r plot pred-resp, fig.height=4, fig.width=6}
ggplot(pred.resp.univar, aes(z, est, ymin = est - 1.96*se, ymax = est + 1.96*se)) + 
    geom_smooth(stat = "identity") + 
    facet_wrap(~ variable)
```

Two-variable predictor-response function for other predictors fixed at their 50th percentile.

```{r pred-resp2, message=FALSE}
pred.resp.bivar <- PredictorResponseBivar(fit = fitkm, y = y, Z = Z, X = X, 
                                          min.plot.dist = 1)
```

Sometimes the image plots are hard to visualize:
```{r plot pred-resp2, fig.height=5, fig.width=6.5}
ggplot(pred.resp.bivar, aes(z1, z2)) + 
    geom_raster(aes(fill = est)) + 
    facet_grid(variable1 ~ variable2)
```

Another option for investigating bivariate predictor-response relations is to plot the predictor-response function of a single predicor in `Z` for the second predictor in `Z` fixed at various quantiles.
```{r pred-resp2 opt2, message=FALSE}
pred.resp.bivar.levels <- PredictorResponseBivarLevels(pred.resp.df = pred.resp.bivar, 
                                                       Z = Z, qs = c(0.25, 0.5, 0.75))
```

```{r plot pred-resp2 opt2, fig.height=5, fig.width=6.5, warning=FALSE}
ggplot(pred.resp.bivar.levels, aes(z1, est)) + 
    geom_smooth(aes(col = quantile), stat = "identity") + 
    facet_grid(variable1 ~ variable2) +
    ggtitle("f(z1 | quantiles of z2)")
```

## Prior distributions for the `r_m` parameters

The BKMR fit depends on the specification of prior distributions. When we ran the `kmbayes` function to fit the model, we didn't change any of the default settings, so the prior specificatios were used. 

The prior distributions can be specified as part of the `control.parameters` argument.
```{r control.params}
fitkm$control.parameters
```

The model fit can be sensitive to the choice of the prior distribution on the `r_m` parameters. To investigate the impact of these parameters on the model fit, we can use the `InvestigatePrior` function.

```{r investigate prior}
priorfits <- InvestigatePrior(y = y, Z = Z, X = X)
```

Here we plot the estimated univariate predictor-response relationships for `r_m` fixed to different values.
```{r investigate prior 2, fig.height=6, fig.width=6}
PlotPriorFits(y = y, Z = Z, X = X, 
              fits = priorfits, which.q = c(1,3,4,6))
```































